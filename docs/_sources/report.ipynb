{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dc3bd70-b851-4971-b8fd-32569fb616d0",
   "metadata": {},
   "source": [
    "# Project Report\n",
    "\n",
    "## Salary predictor for tech employees in Canada based on survey data</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad5f963-565d-44a2-86f0-efa177fb7c47",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "Here we attempt to build the model to predict the income of tech employees in Canada by using a multi-linear regression model based on the following features: years of coding experience, programming languages used, education level, and role. After the hyperparameter tuning process, $R^2$ of the training data set increases from 0.67 to 0.72, and the model is also tested on the testing data set with $R^2 = 0.71$, which is consistent with the training result. Besides, there are 3 points that we want to explore further in the future: to find other explanatory variables that might give us a better score, to include the United States in our model, to identify the best features that contribute to the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a99f14-8e32-4053-8f1a-b9e4c2cf1ef8",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "The aim of this project is to allow tech employees in Canada to get a reasonable estimation of how much they will potentially earn given their skill set and years of experience. Fresh graduates and seasoned employees would benefit from an analysis tool that allows them to predict their earning potential. While the Human Resources (HR) department of companies has access to this market information, tech employees are mostly clueless about what their market value is. Therefore, a salary predictor tool could assist them in the negotiation process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f98b623-45a7-41cd-a66f-7cdffdb94cde",
   "metadata": {},
   "source": [
    "### Methods\n",
    "#### Data\n",
    "The data set used in this project is from the [Stack Overflow Annual Developer Survey](https://insights.stackoverflow.com/survey), which is conducted annually. The survey data set has nearly 80,000 responses. There are several useful features that could be extracted from this survey such as education level, location, the language used, job type, all of which are potentially associated with annual compensation {cite:p}`stack_overflow_survey`.\n",
    "\n",
    "#### Exploratory Data Analysis\n",
    "After performing EDA on the training data set,  there are several points worth mentioning. The distribution of the response variable, salary, is positively skewed with a fat tail, as shown in Fig. 1 {cite:p}`vanderplas2018altair`. This attribute is undesirable, which makes the model less robust. So, extremely high salaries (top 8%) in our training data set will be defined as outliers that are removed in the preprocessing step.<br>\n",
    "\n",
    "```{figure} ../results/salary_density_plot.png\n",
    "---\n",
    "height: 400px\n",
    "name: salary_density\n",
    "---\n",
    "Density plot of annual compensation(USD): The distribution of annual compensation is right skewed with extremely large value as well as extremely small value.\n",
    "```\n",
    "\n",
    "Among all the features investigated, it can be found that the salary is strongly correlated to the number of professional coding years. Fig.2 clearly shows that there is a linear relationship between the number of professional coding years and the salary. Figure.3 displays both effects of professional coding years and languages mastered on the salary.\n",
    "\n",
    "```{figure} ../results/code_years_plot.png\n",
    "---\n",
    "height: 400px\n",
    "name: code_years_plot\n",
    "---\n",
    "Coding year vs. annual compensation(USD): Number of coding years is strongly correlated to compensation, but becomes widely spread when the coding years are greater than 20.\n",
    "```\n",
    "\n",
    "```{figure} ../results/language_codeyears_plot.png\n",
    "---\n",
    "height: 150px\n",
    "name: language_code_years_plot\n",
    "---\n",
    "Both effects of languages mastered and coding years on annual compensation: The more languages mastered and the more years in professional experience, the higher compensation expected.\n",
    "```\n",
    "\n",
    "Figures below present how other 3 features we selected have significant effects on the income level.\n",
    "```{figure} ../results/edu_plot.png\n",
    "---\n",
    "height: 150px\n",
    "name: edu_plot\n",
    "---\n",
    "Education level vs. annual compensation(USD): Education levels are positively related to compensation.\n",
    "```\n",
    "```{figure} ../results/language_plot.png\n",
    "---\n",
    "height: 150px\n",
    "name: lang_plot\n",
    "---\n",
    "Programming languages vs. annual compensation(USD): Programming languages is associated with compensation. The more programming languages mastered, the higher compensation.\n",
    "```\n",
    "```{figure} ../results/role_plot.png\n",
    "---\n",
    "height: 150px\n",
    "name: role_plot\n",
    "---\n",
    "Roles vs. annual compensation(USD): Roles are related to compensation. Mobile and back-end developers have greater compensation.\n",
    "```\n",
    "#### Model\n",
    "In light of EDA and recommendations from Stack Overflow, 4 features are extracted that are duration for being a profession, education level, programming language worked with and job position. Then, the regression equation can be obtained:<br>\n",
    "\n",
    "$$ \n",
    "    y_{salary} = w^T X + b\n",
    "$$\n",
    "\n",
    "*where w is the weight vector, X is the feature vector, b is the error term, $y_{salary}$ is predicted variable.* <br>\n",
    "\n",
    "Ridge model is chosen in this case, which can help reduce over-fitting problems. Ridge solves a regression model where the loss function is the linear least squares function and regularization is given by the L2-norm. \n",
    "\n",
    "$$\n",
    "    loss\\ function = ||y - w^T X||^2_2 + alpha * ||w||^2_2\n",
    "$$\n",
    "\n",
    "*where alpha is regularization strength to reduces the variance of the estimates. Larger values specify stronger regularization.* <br>\n",
    "\n",
    "Within the training data set, randomized hyperparameter searching was carried out based on the scoring matrix, $R^2$. Then, the model with the best performed parameter was used to make prediction on the test data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c00774a-7396-43d9-ab50-d7ee4ebb145f",
   "metadata": {},
   "source": [
    "### Results and Discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ce92dc1-439f-4eb7-b1d2-28e83d50f467",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/supernova/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py:310: UserWarning: Trying to unpickle estimator SimpleImputer from version 1.0.1 when using version 0.24.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "/Users/supernova/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py:310: UserWarning: Trying to unpickle estimator StandardScaler from version 1.0.1 when using version 0.24.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "/Users/supernova/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py:310: UserWarning: Trying to unpickle estimator Pipeline from version 1.0.1 when using version 0.24.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "/Users/supernova/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py:310: UserWarning: Trying to unpickle estimator OneHotEncoder from version 1.0.1 when using version 0.24.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "/Users/supernova/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py:310: UserWarning: Trying to unpickle estimator ColumnTransformer from version 1.0.1 when using version 0.24.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "/Users/supernova/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py:310: UserWarning: Trying to unpickle estimator Ridge from version 1.0.1 when using version 0.24.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "/Users/supernova/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py:310: UserWarning: Trying to unpickle estimator RandomizedSearchCV from version 1.0.1 when using version 0.24.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.091"
      ]
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "",
       "name": "alpha_coef"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.725"
      ]
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "",
       "name": "R2"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.718"
      ]
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "",
       "name": "R2_test"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from joblib import dump, load\n",
    "import pandas as pd\n",
    "from myst_nb import glue\n",
    "import altair as alt\n",
    "from altair_saver import save\n",
    "alt.data_transformers.enable('data_server')\n",
    "alt.renderers.enable('mimetype')\n",
    "\n",
    "# Load regression training results\n",
    "pipe_loaded = load('../results/best_model_pipe.joblib')\n",
    "alpha = round(pipe_loaded.best_params_['ridge__alpha'], 3)\n",
    "rsquare = round(pipe_loaded.best_score_, 3)\n",
    "glue(\"alpha_coef\", alpha);\n",
    "glue(\"R2\", rsquare);\n",
    "\n",
    "# Load regression testing results\n",
    "test_result_loaded = load('../results/test_result.joblib')\n",
    "rsquare_test = round(test_result_loaded[\"r_sq_test\"], 3)\n",
    "glue(\"R2_test\", rsquare_test);\n",
    "\n",
    "# Draw the predicted value error plot\n",
    "test_df = pd.read_csv(\"../data/processed/test.csv\")\n",
    "y_test = test_df.ConvertedComp.tolist()\n",
    "y_predict = test_result_loaded[\"predict_y\"].tolist()\n",
    "result = {\"true_y\": y_test, \"predicted_y\": y_predict}\n",
    "df_result = pd.DataFrame(data=result)\n",
    "df_result.head(5)\n",
    "\n",
    "df_diag = pd.DataFrame(data={\"true_y\": [0, max(df_result.true_y)+500],\n",
    "                             \"predicted_y\":[0, max(df_result.true_y)+500]})\n",
    "\n",
    "plt1 = alt.Chart(df_result).mark_point(opacity=0.5).encode(\n",
    "    alt.X(\"predicted_y\", title=\"Predicted salary\"),\n",
    "    alt.Y(\"true_y\", title=\"True salary\")\n",
    ") + alt.Chart(df_diag).mark_line(color='red').encode(\n",
    "    alt.X(\"predicted_y\", title=\"Predicted salary\"),\n",
    "    alt.Y(\"true_y\", title=\"True salary\")\n",
    ")\n",
    "plt1.save(\"../results/test_data_result.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dc6b8672-035c-4765-b4a9-f26a892375fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/supernova/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py:310: UserWarning: Trying to unpickle estimator SimpleImputer from version 1.0.1 when using version 0.24.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "/Users/supernova/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py:310: UserWarning: Trying to unpickle estimator StandardScaler from version 1.0.1 when using version 0.24.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "/Users/supernova/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py:310: UserWarning: Trying to unpickle estimator Pipeline from version 1.0.1 when using version 0.24.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "/Users/supernova/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py:310: UserWarning: Trying to unpickle estimator OneHotEncoder from version 1.0.1 when using version 0.24.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "/Users/supernova/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py:310: UserWarning: Trying to unpickle estimator ColumnTransformer from version 1.0.1 when using version 0.24.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "/Users/supernova/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py:310: UserWarning: Trying to unpickle estimator Ridge from version 1.0.1 when using version 0.24.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LanguageWorkedWith_Java;R;SQL</td>\n",
       "      <td>296106.070558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LanguageWorkedWith_Bash/Shell/PowerShell;C++;G...</td>\n",
       "      <td>219395.339727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LanguageWorkedWith_Assembly;Bash/Shell/PowerSh...</td>\n",
       "      <td>219145.599062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LanguageWorkedWith_Bash/Shell/PowerShell;HTML/...</td>\n",
       "      <td>178592.285071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LanguageWorkedWith_C#;Java;JavaScript;Python;S...</td>\n",
       "      <td>150506.398681</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             feature    coefficient\n",
       "0                      LanguageWorkedWith_Java;R;SQL  296106.070558\n",
       "1  LanguageWorkedWith_Bash/Shell/PowerShell;C++;G...  219395.339727\n",
       "2  LanguageWorkedWith_Assembly;Bash/Shell/PowerSh...  219145.599062\n",
       "3  LanguageWorkedWith_Bash/Shell/PowerShell;HTML/...  178592.285071\n",
       "4  LanguageWorkedWith_C#;Java;JavaScript;Python;S...  150506.398681"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_best = load('../results/pipe_best.joblib')\n",
    "feat_names = load('../results/feat_names.joblib')\n",
    "coef_list = pipe_best.named_steps[\"ridge\"].coef_\n",
    "\n",
    "df = pd.DataFrame(data = {'feature': feat_names, 'coefficient':coef_list})\n",
    "df.sort_values('coefficient', ascending=False, inplace=True)\n",
    "df.index = list(range(len(df)))\n",
    "df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ef9f9eb8-2bc8-44f2-97aa-f53565ee127f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LanguageWorkedWith_Java;R;SQL\n",
      "LanguageWorkedWith_Bash/Shell/PowerShell;C++;Go;JavaScript;Python;Rust;Scala\n",
      "LanguageWorkedWith_Assembly;Bash/Shell/PowerShell;C;Dart;Go;HTML/CSS;Java;JavaScript;Python;Ruby;SQL\n",
      "LanguageWorkedWith_Bash/Shell/PowerShell;HTML/CSS;JavaScript;Python;R;SQL;Other(s):\n",
      "LanguageWorkedWith_C#;Java;JavaScript;Python;SQL;Swift\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(df.feature[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "40e1dca1-78b8-4105-8bab-8977a11a5c41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ColumnTransformer(transformers=[('pipeline-1',\n",
       "                                 Pipeline(steps=[('simpleimputer',\n",
       "                                                  SimpleImputer()),\n",
       "                                                 ('standardscaler',\n",
       "                                                  StandardScaler())]),\n",
       "                                 ['YearsCodePro']),\n",
       "                                ('pipeline-2',\n",
       "                                 Pipeline(steps=[('simpleimputer',\n",
       "                                                  SimpleImputer(fill_value='missing',\n",
       "                                                                strategy='constant')),\n",
       "                                                 ('onehotencoder',\n",
       "                                                  OneHotEncoder(handle_unknown='ignore',\n",
       "                                                                sparse=False))]),\n",
       "                                 ['DevType', 'EdLevel', 'LanguageWorkedWith'])])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_best.named_steps['columntransformer']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2354c2ff-cbd5-445f-a8f6-fa854c7837cb",
   "metadata": {},
   "source": [
    "As mentioned previously, Ridge model is selected in order to avoid conditioning problems and large estimator coefficients. Firstly, hypeparameterization of alpha is carried out. The hyperparameter tuning result shows that the model is at the best performance when alpha = {glue:text}`alpha_coef` with a training $R^2$  of {glue:text}`R2` as shown in the figure below.\n",
    "\n",
    "```{figure} ../results/alpha-tuning.png\n",
    "---\n",
    "height: 400px\n",
    "name: alpha-tuning\n",
    "---\n",
    "Hyperparameter searching: the model is well trained when the hyperparameter, alpha, equals to 0.091. With a greater alpha, the model is under-fitted, whereas the model is over-fitted if alpha is getting smaller than 0.091.\n",
    "```\n",
    "Applying the fitted model to the test data set, we get a testing $R^2$ of {glue:text}`R2_test`.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0749a47-16f1-4fcb-bc27-d2c69c0f9892",
   "metadata": {},
   "source": [
    "After identifying the most important features, we built multiple linear regression models with the annual salary as our response variable and the following predictors: years of coding experience, programming languages used, education level, and role. Since our target is a continuous variable, regression made sense here.<br>\n",
    "\n",
    "We carried out hyper-parameter tuning via cross validation with `RandomizedSearchCV`. This allowed us to find optimal parameters which improved our validation score from 67% to 72%. We tested the final model on our test data (20% of the survey data) and the model performed well on the test data with an accuracy of 71%. As you can see in Fig 8, the model is slightly under predicting or over-predicting, but the fit seems to be good. This is a decent score that indicates that our model generalizes enough and should perform well on unseen examples.\n",
    "\n",
    "```{figure} ../results/test_data_result.png\n",
    "---\n",
    "height: 400px\n",
    "name: test_data_result\n",
    "---\n",
    "Predicted salary VS. observed salary: the fitted model can fairly predict the salary.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70b7982-2be4-4893-b84f-c9c714357c7c",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "In brief, our linear regression model uses 4 features (that are years of coding experience, programming languages used, education level, and role) to predict salary. It predicts with a $R^2$ score of {glue:text}`R2_test` on the test data set, which performs fairly well. Meanwhile, our work has shown that the salary is closely related to these 4 features. \n",
    "\n",
    "However, there are still limitations in our work. We just used a small part of data from the survey and we are inclined to explore more features to boost the robustness and accuracy of our prediction results. Hence, in the future, we plan to do two important changes; exploring other explanatory variables that might give us a better score, and including the United States in our model. In order to identify the best features that contribute to the prediction, we will include all the sensible columns in the original survey, and perform model and feature selection. We hope that this will help us discover more features that are important for the annual compensation prediction of tech employees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8343cec0-c095-43f5-af0f-8b3e902ba9e4",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "```{bibliography}\n",
    ":style: unsrtalpha\n",
    ":all:\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620a44a1-956a-4e12-8bb9-b095a4ab46f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
